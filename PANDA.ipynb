{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355de001-5b79-490e-b0aa-8fff37bc8433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb88ec03-3b23-4004-9f89-8a735045f2c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = random.randint(1,50)\n",
    "with open('prompts.txt', 'r') as f:\n",
    "    text_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46c0e1d-8c6f-47bf-b136-be1ea6a68956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a3b8bde-46ec-482e-9683-2372b0ec4871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "next_words = []\n",
    "for line in text_data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence[:-1])\n",
    "        next_words.append(n_gram_sequence[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c93b78-c88b-418a-a39a-51cda48068b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(\n",
    "    pad_sequences(\n",
    "    input_sequences, \n",
    "    maxlen=max_sequence_len,\n",
    "    padding='pre')\n",
    ")\n",
    "\n",
    "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4dea2af-f89c-4a26-b056-48c88a3f1d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_filename = 'PANDA.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5841de-58c7-4c90-83ff-6716bd0c95c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(model_filename):\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words,\n",
    "                        60,\n",
    "                        input_length=max_sequence_len - 1))\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(predictors, \n",
    "              label, epochs=300, \n",
    "              verbose=1)\n",
    "    model.save(model_filename)\n",
    "else:\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        total_words, 60, \n",
    "        input_length=max_sequence_len - 1)\n",
    "    )\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        optimizer='adam', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.load_weights(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9168557a-6338-4308-bbef-6f905d87b298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def completion(seed_text, num_words=tokens):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences(\n",
    "        [token_list], \n",
    "        maxlen=max_sequence_len - 1,\n",
    "        padding='pre'\n",
    "    )\n",
    "    predicted_words = []\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_index = np.argmax(predicted)\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        predicted_words.append(predicted_word)\n",
    "        token_list = np.append(\n",
    "            token_list[:, 1:], \n",
    "            [[predicted_index]], \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    response = ' '.join(predicted_words)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "450fd57f-faac-4be2-81e5-215600145271",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def typed(response):\n",
    "    for char in response:\n",
    "        sys.stdout.write(char)\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(0.01)\n",
    "    sys.stdout.write('\\n')\n",
    "    \n",
    "os.system('cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1432441e-e692-42ce-afd9-9392c08ef522",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm PANDA , Paradigm-based Artificial Neural Dialogue Agent , A Language Model which is able to predict next words\n"
     ]
    }
   ],
   "source": [
    "print('I\\'m PANDA , Paradigm-based Artificial Neural Dialogue Agent , A Language Model which is able to predict next words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f7014-b681-4d0e-9e62-113875b072aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "username :  vivek09thakur\n",
      "\n",
      " ↳ (vivek09thakur)  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is a certainly game it ground you've find a good hobby so the husband new new 1 before him about new\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " ↳ (vivek09thakur)  how are \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you doing today i'm doing great what about she only it it was hot a stamp on 20 year in a\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " ↳ (vivek09thakur)  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is it it's only 10 minutes in a hot day at the same relax to win by one human i\n"
     ]
    }
   ],
   "source": [
    "user = input('username : ')\n",
    "while True:\n",
    "    user_input = input(f\"\\n ↳ ({user}) \")\n",
    "    response = completion(user_input)\n",
    "    typed(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c8ef6-3fbd-4e26-a858-ce145bb7a669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
